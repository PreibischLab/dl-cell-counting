{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for testing the posibilities of the data augmentation in keras. The functionality will added to the 2D/3D networks once the tests are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import sys,os,time,random\n",
    "from os import walk\n",
    "import pickle\n",
    "from __future__ import print_function\n",
    "\n",
    "# scientific computing\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg');\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('jet');\n",
    "import skimage\n",
    "from skimage.io import imread, imsave\n",
    "import scipy\n",
    "\n",
    "# machine learning stuff\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"keras\", keras.__version__)\n",
    "print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the backend the ordering of the channels\n",
    "print(keras.backend.backend())\n",
    "print(keras.backend.image_dim_ordering())\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this part is necessary to set the params from the command line\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "if len(sys.argv) == 3 and sys.argv[1] == \"-f\": #on jupyter\n",
    "    sys.argv = ['']\n",
    "    \n",
    "parser = argparse.ArgumentParser(description='Count-ception')\n",
    "\n",
    "parser.add_argument('-seed', type=int, nargs='?',default=0, help='random seed for split and init')\n",
    "parser.add_argument('-data', type=str, nargs='?',default=\"cells\", help='Dataset folder')\n",
    "parser.add_argument('-framesize', type=int, nargs='?',default=256, help='Size of the images processed at once')\n",
    "parser.add_argument('-batchsize', type=int, nargs='?',default=32, help='Size of the batch in the generator')\n",
    "parser.add_argument('-numbatches', type=int, nargs='?',default=5, help='Num of batches in the generator')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how much to extend the initial image \n",
    "patch_size = int(32)\n",
    "framesize = int(args.framesize)\n",
    "framesize_h = framesize_w = framesize\n",
    "channels = int(3)\n",
    "batch_size = int(args.batchsize)\n",
    "num_batches = int(args.numbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "seed = args.seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# piclked names\n",
    "paramfilename = str(patch_size) + \"-\" + str(framesize) + \"-\" + str(batch_size) + \"-\" + str(num_batches) + \"-\" + args.data + \"-generator-params.p\"\n",
    "datasetfilename = str(patch_size) + \"-\" + str(framesize) + \"-\" + str(batch_size) + \"-\" + str(num_batches) + \"-\" + args.data + \"-generator-dataset.p\"\n",
    "print(paramfilename)\n",
    "print(datasetfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input shape is the image shape without the patches\n",
    "input_shape = (framesize, framesize, channels)\n",
    "ext_shape = (input_shape[0] + patch_size, input_shape[0] + patch_size, 1)\n",
    "print (input_shape, ext_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the files from the test folder \n",
    "import glob\n",
    "\n",
    "# prefix = '/Users/kkolyva/'\n",
    "prefix = '/home/milkyklim/'\n",
    "folder = prefix + 'dl-cell-counting/algorithm/data/test-cells'\n",
    "folder_images = \"\" # 'images/'\n",
    "folder_labels = \"\" # 'labels/'\n",
    "img_ext = '.png'\n",
    "\n",
    "print('Images path:', folder + folder_images)\n",
    "print('Labels path:', folder + folder_labels)\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for filename in glob.iglob(folder + folder_labels + \"/*dots\" + img_ext):\n",
    "    imgg = filename.replace(folder_labels + \"dots\", folder_images + \"cell\")\n",
    "    imgs.append([imgg,filename])\n",
    "    \n",
    "if len(imgs) == 0:\n",
    "    print(\"Issue with dataset\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in imgs: \n",
    "    if (not os.path.isfile(path[0])):\n",
    "        print(path, \"bad\", path[0])\n",
    "    if (not os.path.isfile(path[1])):\n",
    "        print(path, \"bad\", path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = [] \n",
    "if (os.path.isfile(datasetfilename)):\n",
    "    print(\"reading\", datasetfilename)\n",
    "    dataset = pickle.load(open(datasetfilename, \"rb\" ))\n",
    "else:\n",
    "    dataset_x = []\n",
    "    dataset_l = []\n",
    "    print(len(imgs))\n",
    "    for path in imgs:\n",
    "        imgPath = path[0]\n",
    "        print(imgPath)\n",
    "        img = imread(imgPath)\n",
    "        labelPath = path[1]\n",
    "        markers = imread(labelPath)\n",
    "        \n",
    "        print(\"img shape\", img.shape, \"markers shape\", markers.shape)\n",
    "        \n",
    "        if img.shape[0:2] != (framesize_w,framesize_h):\n",
    "            print(\"!!!! Not adding image because size is\" , img.shape[0:3])\n",
    "        else:\n",
    "            dataset.append((img, markers))        \n",
    "            print(\"img shape\", img.shape, \"markers shape\", markers.shape)\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    print(\"writing\", datasetfilename)\n",
    "    out = open(datasetfilename, \"wb\",0)\n",
    "    pickle.dump(dataset, out)\n",
    "    print(\"dataset size\", len(dataset))        \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab the data from the data set\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "np_dataset_x = np.asarray([d[0] for d in dataset], dtype=np.float32)\n",
    "np_dataset_l = np.asarray([d[1] for d in dataset], dtype=np.float32)\n",
    "\n",
    "# np_dataset_x = np_dataset_x.transpose((0,3,1,2))\n",
    "\n",
    "print(\"np_dataset_x\", np_dataset_x.shape)\n",
    "print(\"np_dataset_l\", np_dataset_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length = len(np_dataset_x)\n",
    "n = length\n",
    "\n",
    "np_dataset_x_train = np_dataset_x[:n]\n",
    "np_dataset_l_train = np_dataset_l[:n]\n",
    "\n",
    "print(\"total number of initial images: \", len(np_dataset_x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot 2 images to check that the labels are correct \n",
    "# DEBUG:\n",
    "plt.Figure(figsize=(5, 3), dpi=300)\n",
    "plt.subplot(121)\n",
    "img1 = np.array(np_dataset_x_train[1, ].reshape(input_shape), dtype=np.float32).astype(np.uint8)\n",
    "plt.imshow(img1, cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(122)\n",
    "img3 = np.array(np_dataset_l_train[1, ].reshape(input_shape), dtype=np.float32).astype(np.uint8)\n",
    "plt.imshow(img3, cmap=plt.get_cmap('gray'))\n",
    "print(np.sum(img3) / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in case of images and labels we have to use the following approach: \n",
    "# create 2 instances with the same arguments: \n",
    "shift = 0.02\n",
    "image_data_gen_args = dict(featurewise_center=False, \n",
    "                             featurewise_std_normalization=False, \n",
    "                             # zca_whitening=True, \n",
    "                             rotation_range=5,\n",
    "                             width_shift_range=shift, \n",
    "                             height_shift_range=shift,\n",
    "                             fill_mode = 'wrap',\n",
    "                             # cval = 0,\n",
    "                             horizontal_flip=True, \n",
    "                             vertical_flip=True)\n",
    "\n",
    "datagen = ImageDataGenerator(**image_data_gen_args)\n",
    "labelgen = ImageDataGenerator(**image_data_gen_args)\n",
    "\n",
    "datagen.fit(np_dataset_x_train, augment=True, seed=seed)\n",
    "labelgen.fit(np_dataset_l_train, augment=True, seed=seed)\n",
    "\n",
    "image_generator = datagen.flow(\n",
    "    np_dataset_x_train,\n",
    "    seed=seed, \n",
    "    batch_size = batch_size,\n",
    "    save_to_dir='data/aug', save_prefix='cell', save_format='png')\n",
    "label_generator = labelgen.flow(\n",
    "    np_dataset_l_train,\n",
    "    seed=seed,\n",
    "    batch_size = batch_size,\n",
    "    save_to_dir='data/aug', save_prefix='dots', save_format='png')\n",
    "\n",
    "j_batch = 0\n",
    "for X_batch, y_batch in zip(image_generator, label_generator):\n",
    "    print(j_batch, \"th batch with the shapes:\", X_batch.shape, y_batch.shape)     \n",
    "    j_batch += 1;\n",
    "    if (j_batch >= num_batches):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
