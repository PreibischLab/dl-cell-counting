{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The implementation of the CNN is taken from J. P. Cohen, H. Z. Lo, and Y. Bengio, “Count-ception: Counting by Fully Convolutional Redundant Counting,” 2017. https://arxiv.org/abs/1703.08710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys,os,time,random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg');\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('jet');\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import skimage\n",
    "from skimage.io import imread, imsave\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KERAS stuff \n",
    "from __future__ import print_function\n",
    "import keras \n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"keras\", keras.__version__)\n",
    "print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the backend the ordering of the channels\n",
    "print(keras.backend.backend())\n",
    "print(keras.backend.image_dim_ordering())\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup for the gpu: \n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the output of the command above\n",
    "tf.device(\"/gpu:0\")\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# this pone should help with the images of the large size\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this part is necessary to set the params from the command line\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "if len(sys.argv) == 3 and sys.argv[1] == \"-f\": #on jupyter\n",
    "    sys.argv = ['']\n",
    "    \n",
    "parser = argparse.ArgumentParser(description='Count-ception')\n",
    "\n",
    "parser.add_argument('-seed', type=int, nargs='?',default=0, help='random seed for split and init')\n",
    "parser.add_argument('-nsamples', type=int, nargs='?',default=32, help='Number of samples (N) in train and valid')\n",
    "# TODO: Is it used ? \n",
    "parser.add_argument('-stride', type=int, nargs='?',default=1, help='The args.stride at the initial layer')\n",
    "# TODO: Is it used ?\n",
    "parser.add_argument('-lr', type=float, nargs='?',default=0.00005, help='This will set the learning rate ')\n",
    "parser.add_argument('-kern', type=str, nargs='?',default=\"sq\", help='This can be gaus or sq')\n",
    "parser.add_argument('-cov', type=float, nargs='?',default=1, help='This is the covariance when kern=gaus')\n",
    "parser.add_argument('-scale', type=int, nargs='?',default=1, help='Scale the input image and labels')\n",
    "parser.add_argument('-data', type=str, nargs='?',default=\"cells\", help='Dataset folder')\n",
    "parser.add_argument('-framesize', type=int, nargs='?',default=256, help='Size of the images processed at once')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the passed parameters here if you forgot them\n",
    "args.framesize = 256\n",
    "args.scale = 1\n",
    "args.nsamples = 32\n",
    "\n",
    "print(args)\n",
    "print(keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how much to extend the initial image \n",
    "patch_size = int(32)\n",
    "framesize = int(args.framesize/args.scale)\n",
    "channels = int(3)\n",
    "framesize_h = framesize_w = framesize\n",
    "noutputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramfilename = str(args.scale) + \"-\" + str(patch_size) + \"-\" + args.data + \"-\" + args.kern + str(args.cov) + \"_params.p\"\n",
    "datasetfilename = str(args.scale) + \"-\" + str(patch_size) + \"-\" + str(framesize) + \"-\" + args.kern + str(args.stride) + \"-\" + args.data + \"-\" + str(args.cov) + \"-dataset.p\"\n",
    "print(paramfilename)\n",
    "print(datasetfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reproducibility\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "tf.set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input shape is the image shape without the pathces\n",
    "input_shape = (framesize, framesize, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape[0] + patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom loss funciton\n",
    "def mae_loss(y_true, y_pred):\n",
    "    # mae_loss might be too \"greedy\" and train the network on artifacts\n",
    "    # prediction_count2 = np.sum(y_pred / ef)\n",
    "    # mae_loss = K.sum(K.abs(prediction_count2 - (y_true/ef)))\n",
    "    #Mean Absolute Error is computed between each count of the count map\n",
    "    l1_loss = K.abs(y_pred - y_true)\n",
    "    loss = K.mean(l1_loss)    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# custom layers (building blocks)\n",
    "def ConvFactory1(data, num_filters, filter_size, stride=1, pad=0, nonlinearity=LeakyReLU(alpha=0.3)):\n",
    "    # data is the input tensor, leaky rely as a nonlinearity\n",
    "    # the padding is done in the first layer automatically! \n",
    "    # no need to preprocess the data\n",
    "    data = ZeroPadding2D(padding = (pad, pad), data_format=None, input_shape=input_shape)(data)\n",
    "    data = Conv2D(filters = num_filters, kernel_size = (filter_size, filter_size), kernel_initializer='glorot_uniform')(data)\n",
    "    data = LeakyReLU(alpha=0.3)(data)\n",
    "    data = BatchNormalization()(data)\n",
    "    return data\n",
    "    \n",
    "def SimpleFactory1(data, ch_1x1, ch_3x3):\n",
    "    # used for double layers \n",
    "    conv1x1 = ConvFactory1(data, filter_size=1, pad=0, num_filters=ch_1x1)\n",
    "    conv3x3 = ConvFactory1(data, filter_size=3, pad=1, num_filters=ch_3x3) \n",
    "    concat = Concatenate()([conv1x1, conv3x3])\n",
    "    return concat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: check again?\n",
    "def create_model(input_shape):\n",
    "    main_input = Input(shape=input_shape, name='main_input')\n",
    "    # print (net.shape)\n",
    "    net = ConvFactory1(main_input, num_filters=64, pad=patch_size, filter_size = 3)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 16, ch_3x3 = 16)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 16, ch_3x3 = 32)\n",
    "    print (net.shape)\n",
    "    net = ConvFactory1(net, num_filters=16, filter_size = 14)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 112, ch_3x3 = 48)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 40, ch_3x3 = 40)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 32, ch_3x3 = 96)\n",
    "    print (net.shape)\n",
    "\n",
    "    net = ConvFactory1(net, num_filters=16, filter_size = 18)\n",
    "    print (net.shape) \n",
    "    net = ConvFactory1(net, num_filters=64, filter_size = 1)\n",
    "    print (net.shape) \n",
    "    net = ConvFactory1(net, num_filters=64, filter_size = 1)\n",
    "    print (net.shape) \n",
    "    main_output = ConvFactory1(net, filter_size=1, num_filters=1)\n",
    "    print (main_output.shape)\n",
    "    \n",
    "    model = Model(inputs=[main_input], outputs = main_output)  \n",
    "    # mean_absolute_error\n",
    "    model.compile(loss=mae_loss, optimizer='sgd', metrics=['accuracy'])\n",
    "         \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_model(input_shape = input_shape)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sanity check\n",
    "print(\"network output size should be\", (input_shape[0]+2*patch_size)-(patch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (args.kern == \"sq\"):\n",
    "    ef = ((patch_size/args.stride)**2.0)\n",
    "elif (args.kern == \"gaus\"):\n",
    "    ef = 1.0\n",
    "print(\"ef\", ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test run\n",
    "train_start_time = time.time()\n",
    "model.fit(np.zeros([1, input_shape[0], input_shape[1], input_shape[2]]), \n",
    "          np.zeros([1, input_shape[0] + patch_size, input_shape[1] + patch_size, 1]))\n",
    "print(time.time() - train_start_time, \"sec\")\n",
    "\n",
    "train_start_time = time.time()\n",
    "# model.predict();\n",
    "print(time.time() - train_start_time, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fucntions to see the results \n",
    "def genGausImage(framesize, mx, my, cov=1):\n",
    "    x, y = np.mgrid[0:framesize, 0:framesize]\n",
    "    pos = np.dstack((x, y))\n",
    "    mean = [mx, my]\n",
    "    cov = [[cov, 0], [0, cov]]\n",
    "    rv = scipy.stats.multivariate_normal(mean, cov).pdf(pos)\n",
    "    return rv/rv.sum()\n",
    "\n",
    "def getDensity(width, markers):\n",
    "    gaus_img = np.zeros((width,width))\n",
    "    for k in range(width):\n",
    "        for l in range(width):\n",
    "            if (markers[k,l] > 0.5):\n",
    "                gaus_img += genGausImage(len(markers),k-patch_size/2,l-patch_size/2,cov)\n",
    "    return gaus_img\n",
    "\n",
    "def getMarkersCells(labelPath, scale, size):  \n",
    "    labs = imread(labelPath)\n",
    "    if len(labs.shape) == 2:\n",
    "        lab = labs[:,:]/255\n",
    "    elif len(labs.shape) == 3:\n",
    "        lab = labs[:,:,0]/255\n",
    "    else:\n",
    "        print(\"unknown label format\")\n",
    "    \n",
    "    binsize = [scale,scale]\n",
    "    out = np.zeros(size)\n",
    "    for i in range(binsize[0]):\n",
    "        for j in range(binsize[1]):\n",
    "            out = np.maximum(lab[i::binsize[0], j::binsize[1]], out)\n",
    "        \n",
    "    print(lab.sum(),out.sum())\n",
    "    assert np.allclose(lab.sum(),out.sum(), 1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def getCellCountCells(markers, x,y,h,w):\n",
    "    types = [0] * noutputs\n",
    "    for i in range(noutputs):\n",
    "        types[i] = (markers[y:y+h,x:x+w] == 1).sum()\n",
    "        #types[i] = (markers[y:y+h,x:x+w] != -1).sum()\n",
    "    return types\n",
    "\n",
    "def getLabelsCells(markers, img_pad, base_x, base_y, stride, scale):\n",
    "    \n",
    "    height = int ((img_pad.shape[0])/args.stride)\n",
    "    width = int ((img_pad.shape[1])/args.stride)\n",
    "    print(\"label size: \", height, width)\n",
    "    labels = np.zeros((noutputs, height, width))\n",
    "    if (args.kern == \"sq\"):\n",
    "        for y in range(0,height):\n",
    "            for x in range(0,width):\n",
    "                count = getCellCountCells(markers, x*args.stride,y*args.stride,patch_size,patch_size)  \n",
    "                for i in range(0,noutputs):\n",
    "                    labels[i][y][x] = count[i]\n",
    "\n",
    "    \n",
    "    elif (args.kern == \"gaus\"):\n",
    "        for i in range(0,noutputs):\n",
    "            labels[i] = getDensity(width, markers[base_y:base_y+width,base_x:base_x+width])\n",
    "    \n",
    "\n",
    "    count_total = getCellCountCells(markers, 0,0,framesize_h+patch_size,framesize_w+patch_size)\n",
    "    return labels, count_total\n",
    "\n",
    "def getTrainingExampleCells(img_raw, framesize_w, framesize_h, labelPath, base_x,  base_y, stride, scale):\n",
    "    \n",
    "    img = img_raw[base_y:base_y+framesize_h, base_x:base_x+framesize_w]\n",
    "    img_pad = np.pad(img[:,:,0], int ((patch_size)/2), \"constant\")\n",
    "    \n",
    "    markers = getMarkersCells(labelPath, scale, img_raw.shape[0:2])\n",
    "    markers = markers[base_y:base_y+framesize_h, base_x:base_x+framesize_w]\n",
    "    markers = np.pad(markers, patch_size, \"constant\", constant_values=-1)\n",
    "    \n",
    "    labels, count  = getLabelsCells(markers, img_pad, base_x, base_y, args.stride, scale)\n",
    "    return img, labels, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the files from the test folder \n",
    "import glob\n",
    "\n",
    "prefix = '/Users/kkolyva/'\n",
    "# prefix = '/home/milkyklim/Documents/other'\n",
    "folder = prefix + 'dl-cell-counting/algorithm/data/test-cells'\n",
    "img_ext = '.png'\n",
    "\n",
    "print('Full path:', folder)\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for filename in glob.iglob(folder + \"/*dots\" + img_ext):\n",
    "    imgg = filename.replace(\"dots\",\"cell\")\n",
    "    imgs.append([imgg,filename])\n",
    "    \n",
    "if len(imgs) == 0:\n",
    "    print(\"Issue with dataset\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to debug the data generation\n",
    "# adjust the size of the images and labels + check the output of the network\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "imgPath,labelPath,x,y = imgs[9][0],imgs[9][1], 0, 0\n",
    "\n",
    "print(imgPath)\n",
    "print(labelPath)\n",
    "\n",
    "im = imread(imgPath)\n",
    "img_raw_raw = im #grayscale\n",
    "\n",
    "img_raw = scipy.misc.imresize(img_raw_raw, (int(img_raw_raw.shape[0]/args.scale), int(img_raw_raw.shape[1]/args.scale)))\n",
    "print(img_raw_raw.shape,\" ->>>>\", img_raw.shape)\n",
    "\n",
    "print(\"img_raw\", img_raw.shape)\n",
    "img, lab, count = getTrainingExampleCells(img_raw, framesize_w, framesize_h, labelPath, x, y, args.stride, args.scale)\n",
    "print(\"count\", count)\n",
    "\n",
    "markers = getMarkersCells(labelPath, args.scale, img_raw.shape[0:2])\n",
    "markers = markers[y:y+framesize_h, x:x+framesize_w]\n",
    "count = getCellCountCells(markers, 0, 0, framesize_w,framesize_h)\n",
    "print(\"count\", count, 'markers max', markers.max())\n",
    "\n",
    "pcount = model.predict(np.array([img]), batch_size=1)\n",
    "\n",
    "lab_est = [(l.sum()/ef).astype(np.int) for l in lab]\n",
    "pred_est = [(l.sum()/ef).astype(np.int) for l in pcount]\n",
    "\n",
    "print(\"img shape\",  img.shape)\n",
    "print(\"label shape\", lab.shape)\n",
    "print(\"label est \",lab_est,\" --> predicted est \",pred_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that the plotting setup is working \n",
    "fig = plt.Figure(figsize=(18, 9), dpi=160)\n",
    "gcf = plt.gcf()\n",
    "gcf.set_size_inches(18, 15)\n",
    "fig.set_canvas(gcf.canvas)\n",
    "\n",
    "ax2 = plt.subplot2grid((2,4), (0, 0), colspan=2)\n",
    "ax3 = plt.subplot2grid((2,4), (0, 2), colspan=3)\n",
    "ax4 = plt.subplot2grid((2,4), (1, 2), colspan=3)\n",
    "ax5 = plt.subplot2grid((2,4), (1, 0), rowspan=1)\n",
    "ax6 = plt.subplot2grid((2,4), (1, 1), rowspan=1)\n",
    "\n",
    "ax2.set_title(\"Input Image\")\n",
    "ax2.imshow(img, interpolation='none',cmap='Greys_r');\n",
    "ax3.set_title(\"Regression target, {}x{} sliding window.\".format(patch_size, patch_size))\n",
    "ax3.imshow(np.concatenate((lab),axis=1), interpolation='none');\n",
    "#ax3.imshow(lab[0], interpolation='none')\n",
    "\n",
    "ax4.set_title(\"Predicted counts\")\n",
    "# (pcount),axis=1\n",
    "ax4.imshow(np.concatenate((pcount[0, :, : ,:]),axis=1), interpolation='none');\n",
    "\n",
    "ax5.set_title(\"Real \" + str(lab_est))\n",
    "ax5.set_ylim((0, np.max(lab_est)*2))\n",
    "ax5.set_xticks(np.arange(0, noutputs, 1.0))\n",
    "ax5.bar(range(noutputs),lab_est, align='center')\n",
    "ax6.set_title(\"Pred \" + str(pred_est))\n",
    "ax6.set_ylim((0, np.max(lab_est)*2))\n",
    "ax6.set_xticks(np.arange(0, noutputs, 1.0))\n",
    "ax6.bar(range(noutputs),pred_est, align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# actually learning  pprocess starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in imgs: \n",
    "    if (not os.path.isfile(path[0])):\n",
    "        print(path, \"bad\", path[0])\n",
    "    if (not os.path.isfile(path[1])):\n",
    "        print(path, \"bad\", path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "if (os.path.isfile(datasetfilename)):\n",
    "    print(\"reading\", datasetfilename)\n",
    "    dataset = pickle.load(open(datasetfilename, \"rb\" ))\n",
    "else:\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    dataset_c = []\n",
    "    print(len(imgs))\n",
    "    for path in imgs: \n",
    "\n",
    "        imgPath = path[0]\n",
    "        print(imgPath)\n",
    "\n",
    "        im = imread(imgPath)\n",
    "        img_raw_raw = im\n",
    "        \n",
    "        img_raw = scipy.misc.imresize(img_raw_raw, (int(img_raw_raw.shape[0]/args.scale),int(img_raw_raw.shape[1]/args.scale)))\n",
    "        print(img_raw_raw.shape,\" ->>>>\", img_raw.shape)\n",
    "\n",
    "        labelPath = path[1]\n",
    "        for base_x in range(0,img_raw.shape[0],framesize_h):\n",
    "            for base_y in range(0,img_raw.shape[1],framesize_w):\n",
    "                \n",
    "                if (img_raw.shape[1] - base_y < framesize_w) or (img_raw.shape[0] - base_x < framesize_h):\n",
    "                    print(\"!!!! Not adding image because size is\" , img_raw.shape[1] - base_y, img_raw.shape[0] - base_x)\n",
    "                    continue\n",
    "                    \n",
    "                img, lab, count = getTrainingExampleCells(img_raw, framesize_w, framesize_h, labelPath, base_y, base_x, args.stride, args.scale)\n",
    "                print(\"count \", count)\n",
    "                    \n",
    "                if img.shape[0:2] != (framesize_w,framesize_h):\n",
    "                    print(\"!!!! Not adding image because size is\" , img.shape[0:2])\n",
    "                    \n",
    "                else :   \n",
    "                    lab_est = [(l.sum()/ef).astype(np.int) for l in lab]\n",
    "                \n",
    "                    assert np.allclose(count,lab_est, 0)\n",
    "                \n",
    "                    dataset.append((img,lab,count))\n",
    "                    \n",
    "                    print(\"lab_est\", lab_est, \"img shape\", img.shape, \"label shape\", lab.shape)\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "        print(\"dataset size\", len(dataset))\n",
    "                    \n",
    "    print(\"writing\", datasetfilename)\n",
    "    out = open(datasetfilename, \"wb\",0)\n",
    "    pickle.dump(dataset, out)\n",
    "    out.close()\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab the data from the data set\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "np_dataset_x = np.asarray([d[0] for d in dataset], dtype=np.float32)\n",
    "np_dataset_y = np.asarray([d[1] for d in dataset], dtype=np.float32)\n",
    "np_dataset_c = np.asarray([d[2] for d in dataset], dtype=np.float32)\n",
    "\n",
    "np_dataset_x = np_dataset_x.transpose((0,3,1,2))\n",
    "\n",
    "print(\"np_dataset_x\", np_dataset_x.shape)\n",
    "print(\"np_dataset_y\", np_dataset_y.shape)\n",
    "print(\"np_dataset_c\", np_dataset_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length = len(np_dataset_x)\n",
    "\n",
    "# 2/3 vs 1/3 for training and validation\n",
    "n = int (args.nsamples / 3); \n",
    "\n",
    "np_dataset_x_train = np_dataset_x[0:2*n]\n",
    "np_dataset_y_train = np_dataset_y[0:2*n]\n",
    "np_dataset_c_train = np_dataset_c[0:2*n]\n",
    "print(\"np_dataset_x_train\", len(np_dataset_x_train))\n",
    "\n",
    "np_dataset_x_valid = np_dataset_x[2*n:3*n]\n",
    "np_dataset_y_valid = np_dataset_y[2*n:3*n]\n",
    "np_dataset_c_valid = np_dataset_c[2*n:3*n]\n",
    "print(\"np_dataset_x_valid\", len(np_dataset_x_valid))\n",
    "\n",
    "np_dataset_x_test = np_dataset_x[3*n:]\n",
    "np_dataset_y_test = np_dataset_y[3*n:]\n",
    "np_dataset_c_test = np_dataset_c[3*n:]\n",
    "print(\"np_dataset_x_test\", len(np_dataset_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some stats before the run\n",
    "print(\"number of counts total \", np_dataset_c.sum())\n",
    "print(\"number of counts on average \", np_dataset_c.mean(), \"+-\", np_dataset_c.std())\n",
    "print(\"counts min:\", np_dataset_c.min(), \"max:\", np_dataset_c.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examples of the images from the training set \n",
    "n_images_show = 7\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Example images\")\n",
    "plt.imshow(np.concatenate(np_dataset_x_train[:n_images_show].astype(np.uint8).transpose((0,2,3,1)),axis=1), interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Example images\")\n",
    "plt.imshow(np.concatenate(np_dataset_y_train[:n_images_show,0],axis=1), interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Counts in each image\")\n",
    "plt.bar(range(len(np_dataset_c_train)),np_dataset_c_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some stats again\n",
    "print(\"Total cells in training\", np.sum(np_dataset_c_train[0:], axis=0))\n",
    "print(\"Total cells in validation\", np.sum(np_dataset_c_valid[0:], axis=0))\n",
    "print(\"Total cells in testing\", np.sum(np_dataset_c_test[0:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to make video: ffmpeg -i images-cell/image-0-%d-cell.png -vcodec libx264 aout.mp4\n",
    "def processImages(name, i):\n",
    "    fig = plt.Figure(figsize=(18, 9), dpi=160)\n",
    "    gcf = plt.gcf()\n",
    "    gcf.set_size_inches(18, 15)\n",
    "    fig.set_canvas(gcf.canvas)\n",
    "    \n",
    "    (img, lab, count) = dataset[i]\n",
    "    \n",
    "    #print str(i),count\n",
    "    pcount = model.predict(img[None, :, :, :])[0]    \n",
    "    lab_est = [(l.sum()/(ef)).astype(np.int) for l in lab]\n",
    "    \n",
    "    #print lab_est\n",
    "    \n",
    "    pred_est = np.sum([(l.sum()/(ef)).astype(np.int) for l in pcount[:,:,0]])\n",
    "\n",
    "    print(str(i),\"label est \",lab_est,\" --> predicted est \",pred_est)\n",
    "\n",
    "    ax2 = plt.subplot2grid((2,6), (0, 0), colspan=2)\n",
    "    ax3 = plt.subplot2grid((2,6), (0, 2), colspan=5)\n",
    "    ax4 = plt.subplot2grid((2,6), (1, 2), colspan=5)\n",
    "    ax5 = plt.subplot2grid((2,6), (1, 0), rowspan=1)\n",
    "    ax6 = plt.subplot2grid((2,6), (1, 1), rowspan=1)\n",
    "\n",
    "    ax2.set_title(\"Input Image\")\n",
    "    ax2.imshow(img, interpolation='none', cmap='Greys_r')\n",
    "    ax3.set_title(\"Regression target, {}x{} sliding window.\".format(patch_size, patch_size))\n",
    "    ax3.imshow(np.concatenate((lab),axis=1), interpolation='none')\n",
    "    ax4.set_title(\"Predicted counts\")\n",
    "    ax4.imshow(np.concatenate(pcount.transpose((1,0,2)), axis=1), interpolation='none')\n",
    "\n",
    "    ax5.set_title(\"Real \" + str(lab_est))\n",
    "    ax5.set_ylim((0, np.max(lab_est)*2))\n",
    "    ax5.set_xticks(np.arange(0, noutputs, 1.0))\n",
    "    ax5.bar(range(noutputs),lab_est, align='center')\n",
    "    ax6.set_title(\"Pred \" + str(pred_est))\n",
    "    ax6.set_ylim((0, np.max(lab_est)*2))\n",
    "    ax6.set_xticks(np.arange(0, noutputs, 1.0))\n",
    "    ax6.bar(range(noutputs),pred_est, align='center')\n",
    "    if not os.path.exists('images-cell'): \n",
    "        os.mkdir('images-cell')\n",
    "    fig.savefig('images-cell/image-' + str(i) + \"-\" + name + '.png', bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "processImages('test',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = \"network-temp/\"\n",
    "model_ext = \"-countception.h5py\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "def save_network(net, name):\n",
    "    net.save(directory + str(name) + model_ext)\n",
    "    \n",
    "# pay attention to the loss function    \n",
    "def load_network(name):\n",
    "    return load_model(directory + str(name) + model_ext, custom_objects={'mae_loss': mae_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model if you have one \n",
    "# model = load_network('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: is this one correct ?\n",
    "# test accuracy\n",
    "def test_perf(dataset_x, dataset_y, dataset_c):\n",
    "    testpixelerrors = []\n",
    "    testerrors = []\n",
    "    bs = 1\n",
    "    \n",
    "    # loop is unnecessary! \n",
    "    for i in range(0,len(dataset_x), bs):\n",
    "\n",
    "        pcount = model.predict(dataset_x, batch_size=4)\n",
    "        # pcount = model.predict(dataset_x,range(i,i+bs))\n",
    "        pixelerr = np.abs(pcount - dataset_y[i:i+bs]).mean(axis=(2,3))\n",
    "        testpixelerrors.append(pixelerr)\n",
    "        \n",
    "        pred_est = (pcount/(ef)).sum(axis=(1,2,3))\n",
    "        err = np.abs(dataset_c[i:i+bs].flatten()-pred_est)\n",
    "        \n",
    "        testerrors.append(err)\n",
    "    \n",
    "    return np.abs(testpixelerrors).mean(), np.abs(testerrors).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "batch_size = 4\n",
    "\n",
    "model.fit(np_dataset_x_train.transpose((0,2,3,1)), np_dataset_y_train.transpose((0,2,3,1)),\n",
    "          epochs = num_epochs,\n",
    "          batch_size = batch_size,\n",
    "          validation_data=(np_dataset_x_valid.transpose((0,2,3,1)), np_dataset_y_valid.transpose((0,2,3,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_network(model, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ouput1 = model.predict(np_dataset_x_test.transpose((0,2,3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (ouput1.shape)\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Example images\")\n",
    "plt.imshow(ouput1[2, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: is of any use?\n",
    "print(\"Random performance\")\n",
    "print(test_perf(np_dataset_x_train.transpose((0,2,3,1)), np_dataset_y_train, np_dataset_c_train))\n",
    "print(test_perf(np_dataset_x_valid.transpose((0,2,3,1)), np_dataset_y_valid, np_dataset_c_valid))\n",
    "print(test_perf(np_dataset_x_test.transpose((0,2,3,1)), np_dataset_y_test, np_dataset_c_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_network(model, \"test-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_counts(dataset_x):\n",
    "    batch_size = 1\n",
    "    pcount = model.predict(dataset_x.transpose((0,2,3,1)), batch_size=batch_size)\n",
    "    pred_est = np.sum(pcount / ef, axis=(1,2))   \n",
    "    print(pred_est.shape)\n",
    "    return pred_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Training Data\")\n",
    "\n",
    "pcounts = compute_counts(np_dataset_x_train)\n",
    "plt.bar(np.arange(len(np_dataset_c_train))-0.1,np_dataset_c_train, width=0.5, label=\"Real Count\");\n",
    "plt.bar(np.arange(len(np_dataset_c_train))+0.1,pcounts, width=0.5,label=\"Predicted Count\");\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Valid Data\")\n",
    "\n",
    "pcounts = compute_counts(np_dataset_x_valid)\n",
    "plt.bar(np.arange(len(np_dataset_c_valid))-0.1,np_dataset_c_valid, width=0.5, label=\"Real Count\");\n",
    "plt.bar(np.arange(len(np_dataset_c_valid))+0.1,pcounts, width=0.5,label=\"Predicted Count\");\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Test Data\")\n",
    "\n",
    "pcounts = compute_counts(np_dataset_x_test)\n",
    "plt.bar(np.arange(len(np_dataset_c_test))-0.1,np_dataset_c_test, width=0.5, label=\"Real Count\");\n",
    "plt.bar(np.arange(len(np_dataset_c_test))+0.1,pcounts, width=0.5,label=\"Predicted Count\");\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
