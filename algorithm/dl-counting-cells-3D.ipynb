{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3D extension of the countception network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the CNN is taken from J. P. Cohen, H. Z. Lo, and Y. Bengio, “Count-ception: Counting by Fully Convolutional Redundant Counting,” 2017. https://arxiv.org/abs/1703.08710"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT!** Ensure that notebook was started with the KERAS_BACKEND=tensorflow variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys,os,time,random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg');\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('jet');\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import skimage\n",
    "from skimage.io import imread, imsave\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KERAS stuff \n",
    "from __future__ import print_function\n",
    "import keras \n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.layers.convolutional import ZeroPadding2D, ZeroPadding3D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Merge\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"keras\", keras.__version__)\n",
    "print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the backend the ordering of the channels\n",
    "print(keras.backend.backend())\n",
    "print(keras.backend.image_dim_ordering())\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup for the gpu: \n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "multiple_gpu_available = False\n",
    "if (len(get_available_gpus()) > 1):\n",
    "    multiple_gpu_available = True\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the output of the command above\n",
    "tf.device(\"/gpu:0\")\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# this pone should help with the images of the large size\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this part is necessary to set the params from the command line\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "if len(sys.argv) == 3 and sys.argv[1] == \"-f\": #on jupyter\n",
    "    sys.argv = ['']\n",
    "    \n",
    "parser = argparse.ArgumentParser(description='Count-ception')\n",
    "\n",
    "parser.add_argument('-seed', type=int, nargs='?',default=0, help='random seed for split and init')\n",
    "parser.add_argument('-nsamples', type=int, nargs='?',default=32, help='Number of samples (N) in train and valid')\n",
    "# TODO: Is it used ? \n",
    "parser.add_argument('-stride', type=int, nargs='?',default=1, help='The args.stride at the initial layer')\n",
    "parser.add_argument('-lr', type=float, nargs='?',default=0.00005, help='This will set the learning rate ')\n",
    "parser.add_argument('-kern', type=str, nargs='?',default=\"sq\", help='This can be gaus or sq')\n",
    "parser.add_argument('-cov', type=float, nargs='?',default=1, help='This is the covariance when kern=gaus')\n",
    "parser.add_argument('-scale', type=int, nargs='?',default=1, help='Scale the input image and labels')\n",
    "parser.add_argument('-data', type=str, nargs='?',default=\"cells\", help='Dataset folder')\n",
    "parser.add_argument('-framesize', type=int, nargs='?',default=256, help='Size of the images processed at once')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the passed parameters here if you forgot them\n",
    "args.framesize = 256\n",
    "args.scale = 1\n",
    "args.nsamples = 21\n",
    "\n",
    "print(args)\n",
    "print(keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how much to extend the initial image \n",
    "patch_size_w = int(32)\n",
    "patch_size_h = int(32)\n",
    "patch_size_d = int(32) # in the real example this one should be 8\n",
    "framesize = int(args.framesize/args.scale)\n",
    "channels = int(1)\n",
    "framesize_h = int(256)\n",
    "framesize_w = int(256)\n",
    "framesize_d = int(27) # in the real example this one should be 27\n",
    "noutputs = 1\n",
    "nsamples = args.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramfilename = str(args.scale) + \"-\" + str(patch_size_w) + \"-\" + args.data + \"-\" + args.kern + str(args.cov) + \"_params-3D.p\"\n",
    "datasetfilename = str(args.scale) + \"-\" + str(patch_size_w) + \"-\" + str(framesize) + \"-\" + args.kern + str(args.stride) + \"-\" + args.data + \"-\" + str(args.cov) + \"-dataset-3D.p\"\n",
    "print(paramfilename)\n",
    "print(datasetfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reproducibility\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "tf.set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input shape is the image shape without the pathces\n",
    "input_shape = (framesize_w, framesize_h, framesize_d, channels)\n",
    "ext_shape = (framesize_w + patch_size_w, framesize_h + patch_size_h, framesize_d + + patch_size_d, channels)\n",
    "\n",
    "print(input_shape)\n",
    "print(ext_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the input dimensions for the network\n",
    "[x + y for x, y in zip(input_shape[0:3], (patch_size_w, patch_size_h, patch_size_d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to split the data between multiple GPU's\n",
    "def make_parallel(model, gpu_count):\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        size = tf.concat([ shape[:1] // parts, shape[1:] ],axis=0)\n",
    "        stride = tf.concat([ shape[:1] // parts, shape[1:]*0 ],axis=0)\n",
    "        start = stride * idx\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    inputs.append(slice_n)                \n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(Concatenate(axis=0)(outputs))\n",
    "            \n",
    "        return Model(inputs=model.inputs, outputs=merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom loss funciton\n",
    "def mae_loss(y_true, y_pred):\n",
    "    # mae_loss might be too \"greedy\" and train the network on artifacts\n",
    "    # prediction_count2 = np.sum(y_pred / ef)\n",
    "    # mae_loss = K.sum(K.abs(prediction_count2 - (y_true/ef)))\n",
    "    #Mean Absolute Error is computed between each count of the count map\n",
    "    l1_loss = K.abs(y_pred - y_true)\n",
    "    loss = K.mean(l1_loss)    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom layers (building blocks)\n",
    "def ConvFactory1(data, num_filters, filter_size, stride=1, pad=(0, 0, 0), nonlinearity=LeakyReLU(alpha=0.3)):\n",
    "    # data is the input tensor, leaky rely as a nonlinearity\n",
    "    # the padding is done in the first layer automatically! \n",
    "    # no need to preprocess the data\n",
    "    data = ZeroPadding3D(padding = pad, data_format=None, input_shape=input_shape)(data)\n",
    "    data = Conv3D(filters = num_filters, kernel_size = (filter_size, filter_size, filter_size), kernel_initializer='glorot_uniform')(data)\n",
    "    data = LeakyReLU(alpha=0.3)(data)\n",
    "    data = BatchNormalization()(data)\n",
    "    return data\n",
    "    \n",
    "def SimpleFactory1(data, ch_1x1, ch_3x3):\n",
    "    # used for double layers \n",
    "    conv1x1 = ConvFactory1(data, filter_size=1, pad=0, num_filters=ch_1x1)\n",
    "    conv3x3 = ConvFactory1(data, filter_size=3, pad=1, num_filters=ch_3x3) \n",
    "    concat = Concatenate()([conv1x1, conv3x3])\n",
    "    return concat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: check again?\n",
    "def create_model(input_shape):\n",
    "    main_input = Input(shape=input_shape, name='main_input')\n",
    "    # print (net.shape)\n",
    "    net = ConvFactory1(main_input, num_filters=64, pad=(patch_size_w, patch_size_h, patch_size_d), filter_size = 3)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 16, ch_3x3 = 16)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 16, ch_3x3 = 32)\n",
    "    print (net.shape)\n",
    "    net = ConvFactory1(net, num_filters=16, filter_size = 14)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 112, ch_3x3 = 48)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 40, ch_3x3 = 40)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 32, ch_3x3 = 96)\n",
    "    print (net.shape)\n",
    "\n",
    "    net = ConvFactory1(net, num_filters=16, filter_size = 18)\n",
    "    print (net.shape) \n",
    "    net = ConvFactory1(net, num_filters=64, filter_size = 1)\n",
    "    print (net.shape) \n",
    "    net = ConvFactory1(net, num_filters=64, filter_size = 1)\n",
    "    print (net.shape) \n",
    "    main_output = ConvFactory1(net, filter_size=1, num_filters=1)\n",
    "    print (main_output.shape)\n",
    "    \n",
    "    model = Model(inputs=[main_input], outputs = main_output)\n",
    "    # make the model parallel for 2 gpu's\n",
    "    if multiple_gpu_available:\n",
    "        model = make_parallel(model, 2)\n",
    "    # mean_absolute_error\n",
    "    sgd = SGD(lr=args.lr, nesterov=True)\n",
    "    model.compile(loss=mae_loss, optimizer=sgd, metrics=['accuracy'])\n",
    "         \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_model(input_shape = input_shape)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test memory needed to run the model\n",
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes\n",
    "\n",
    "get_model_memory_usage(2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sanity check\n",
    "print(\"network output size should be\", [x + y for x, y in zip(input_shape[0:3], (patch_size_w, patch_size_h, patch_size_d))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: FIX THE EF COEFFICENT FOR THE CASE OF CUBE \n",
    "if (args.kern == \"sq\"):\n",
    "    ef = (((patch_size_w*patch_size_h*patch_size_d))/args.stride)\n",
    "elif (args.kern == \"gaus\"):\n",
    "    ef = 1.0\n",
    "print(\"ef\", ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test run\n",
    "train_start_time = time.time()\n",
    "# model.fit(np.zeros([1, input_shape[0], input_shape[1], input_shape[2], input_shape[3]]), \n",
    "#          np.zeros([1, input_shape[0] + patch_size_w, input_shape[1] + patch_size_h, input_shape[2] + patch_size_d, 1]))\n",
    "print(time.time() - train_start_time, \"sec\")\n",
    "\n",
    "train_start_time = time.time()\n",
    "# model.predict();\n",
    "print(time.time() - train_start_time, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THIS PART IS NOT WORKING, YOU MIGHT HAVE TO DELETE IT ALL\n",
    "# fucntions to see the results \n",
    "# def genGausImage(framesize, mx, my, mz, cov=1):  \n",
    "#     framesize_x = framesize[0]\n",
    "#     framesize_y = framesize[1]\n",
    "#     framesize_z = framesize[2]\n",
    "    \n",
    "#     x, y, z = np.mgrid[0:framesize_x, 0:framesize_y, 0:framesize_z]\n",
    "#     pos = np.dstack((x, y, z))\n",
    "#     mean = [mx, my, mz]\n",
    "#     cov = [[cov, 0, 0], [0, cov, 0], [0, 0, cov]]\n",
    "#     rv = scipy.stats.multivariate_normal(mean, cov).pdf(pos)\n",
    "#     return rv/rv.sum()\n",
    "\n",
    "# # TODO: FIX!\n",
    "# def getDensity(width, markers):\n",
    "#     gaus_img = np.zeros((width,width))\n",
    "#     for k in range(width):\n",
    "#         for l in range(width):\n",
    "#             if (markers[k,l] > 0.5):\n",
    "#                 gaus_img += genGausImage(len(markers),k-patch_size/2,l-patch_size/2,cov)\n",
    "#     return gaus_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMarkersCells(labelPath, scale, size):  \n",
    "    labs = imread(labelPath).transpose([1,2,0])\n",
    "    if len(labs.shape) == 3:\n",
    "        lab = labs[:,:,:]/255\n",
    "    elif len(labs.shape) == 4:\n",
    "        lab = labs[:,:,:,0]/255\n",
    "    else:\n",
    "        print(\"unknown label format\")\n",
    "    \n",
    "    binsize = [scale, scale, scale]\n",
    "    out = np.zeros(size)    \n",
    "    for i in range(binsize[0]):\n",
    "        for j in range(binsize[1]):\n",
    "            for k in range(binsize[2]):\n",
    "                out = np.maximum(lab[i::binsize[0], j::binsize[1], k::binsize[2]], out)\n",
    "        \n",
    "    print(lab.sum(),out.sum())\n",
    "    assert np.allclose(lab.sum(),out.sum(), 1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def getCellCountCells(markers, x, y, z, h, w, d):\n",
    "    types = [0] * noutputs\n",
    "    for i in range(noutputs):\n",
    "        types[i] = (markers[y:y+h,x:x+w, z:z+w] == 1).sum()\n",
    "        #types[i] = (markers[y:y+h,x:x+w] != -1).sum()\n",
    "    return types\n",
    "\n",
    "def getLabelsCells(markers, img_pad, base_x, base_y, base_z, stride, scale):\n",
    "    \n",
    "    height = int ((img_pad.shape[0])/args.stride)\n",
    "    width = int ((img_pad.shape[1])/args.stride)\n",
    "    depth = int ((img_pad.shape[2])/args.stride)\n",
    "    \n",
    "    print(\"label size: \", height, width, depth)\n",
    "    labels = np.zeros((noutputs, height, width, depth))\n",
    "    if (args.kern == \"sq\"):\n",
    "        for y in range(0,height):\n",
    "            for x in range(0,width):\n",
    "                for z in range(0, depth):\n",
    "                    count = getCellCountCells(markers, x*args.stride, y*args.stride, z*args.stride, patch_size_w, patch_size_h, patch_size_d)  \n",
    "                    for i in range(0,noutputs):\n",
    "                        labels[i][y][x][z] = count[i]\n",
    "\n",
    "    \n",
    "    elif (args.kern == \"gaus\"):\n",
    "        # gauss is not checked in 3D\n",
    "        for i in range(0,noutputs):\n",
    "            labels[i] = getDensity(width, markers[base_y:base_y+width,base_x:base_x+width, base_z:base_z+depth])\n",
    "    \n",
    "    \n",
    "    print(\"getLabelsCells: DONE!\")\n",
    "    \n",
    "    count_total = getCellCountCells(markers, 0, 0, 0, framesize_h+patch_size_h, framesize_w+patch_size_w, framesize_d+patch_size_d)\n",
    "    return labels, count_total\n",
    "\n",
    "def getTrainingExampleCells(img_raw, framesize_w, framesize_h, framesize_d, labelPath, base_x,  base_y, base_z, stride, scale):\n",
    "    \n",
    "    img = img_raw[base_y:base_y+framesize_h, base_x:base_x+framesize_w, base_z:base_z+framesize_d]\n",
    "    img_pad = np.pad(img[:,:,:], ((int ((patch_size_w)/2), int ((patch_size_w)/2)), \n",
    "                                     (int ((patch_size_h)/2), int ((patch_size_h)/2)), \n",
    "                                     (int ((patch_size_d)/2), int ((patch_size_d)/2))), \"constant\")\n",
    "    \n",
    "    markers = getMarkersCells(labelPath, scale, img_raw.shape[0:3])\n",
    "    markers = markers[base_y:base_y+framesize_h, base_x:base_x+framesize_w, base_z:base_z+framesize_d]\n",
    "    markers = np.pad(markers, ((patch_size_w, patch_size_w), \n",
    "                               (patch_size_h, patch_size_h), \n",
    "                               (patch_size_d, patch_size_d)), \"constant\", constant_values=-1)\n",
    "    \n",
    "    labels, count  = getLabelsCells(markers, img_pad, base_x, base_y, base_z, args.stride, scale)\n",
    "    return img, labels, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the files from the test folder \n",
    "import glob\n",
    "\n",
    "# prefix = '/Users/kkolyva/'\n",
    "prefix = '/home/milkyklim/'\n",
    "folder = prefix + 'dl-cell-counting/algorithm/data/2017-10-16-dl-data-dapi/data/run'\n",
    "img_ext = '.tif'\n",
    "\n",
    "print('Full path:', folder)\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for filename in glob.iglob(folder + \"/*dots\" + img_ext):\n",
    "    imgg = filename.replace(\"dots\",\"cell\")\n",
    "    imgp = filename.replace(\"dots\",\"patch\")\n",
    "    imgs.append([imgg,filename, imgp])\n",
    "    \n",
    "if len(imgs) == 0:\n",
    "    print(\"Issue with dataset\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put some /show images/ here to see the result of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in imgs: \n",
    "    if (not os.path.isfile(path[0])):\n",
    "        print(path, \"bad\", path[0])\n",
    "    if (not os.path.isfile(path[1])):\n",
    "        print(path, \"bad\", path[1])\n",
    "    if (not os.path.isfile(path[2])):\n",
    "        print(path, \"bad\", path[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DEBUG: DEV:\n",
    "dataset = [] \n",
    "if (os.path.isfile(datasetfilename) and False):\n",
    "    print(\"reading\", datasetfilename)\n",
    "    dataset = pickle.load(open(datasetfilename, \"rb\" ))\n",
    "else:\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    dataset_c = []\n",
    "    print(len(imgs))\n",
    "    for path in imgs:\n",
    "        imgPath = path[0]\n",
    "        print(imgPath)\n",
    "        img = imread(imgPath).transpose([1,2,0])[:, :, :, None]\n",
    "        labelPath = path[1]\n",
    "        markers = imread(labelPath).transpose([1,2,0])[:, :, :, None]\n",
    "        patchPath = path[2]\n",
    "        lab = imread(patchPath).transpose([1,2,0])[:, :, :, None]\n",
    "            \n",
    "        if (img.shape != input_shape) or (markers.shape != input_shape) or (lab.shape != ext_shape):\n",
    "            print(\"!!!! Not adding image because size is\" , img.shape, markers.shape, lab.shape )\n",
    "        else:\n",
    "            \n",
    "            lab_est = lab[:].sum() / ef\n",
    "            # [(l.sum()/ef).astype(np.int) for l in lab]\n",
    "            print(\"lab_est \", lab_est)\n",
    "            \n",
    "            count = np.sum(markers[:]) // 255\n",
    "            print(\"count \", count)\n",
    "            \n",
    "            assert np.allclose(count,lab_est, 0)\n",
    "            \n",
    "            \n",
    "            dataset.append((img, lab, count))        \n",
    "            print(\"img shape\", img.shape, \"markers shape\", markers.shape, \"lab.shape\", lab.shape)\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    print(\"writing\", datasetfilename)\n",
    "    out = open(datasetfilename, \"wb\",0)\n",
    "    pickle.dump(dataset, out)\n",
    "    print(\"dataset size\", len(dataset))        \n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OBSOLETE!\n",
    "\n",
    "\n",
    "# dataset = []\n",
    "# if (os.path.isfile(datasetfilename)):\n",
    "#     print(\"reading\", datasetfilename)\n",
    "#     dataset = pickle.load(open(datasetfilename, \"rb\" ))\n",
    "# else:\n",
    "#     dataset_x = []\n",
    "#     dataset_y = []\n",
    "#     dataset_c = []\n",
    "#     print(len(imgs))\n",
    "#     for path in imgs: \n",
    "\n",
    "#         imgPath = path[0]\n",
    "#         print(imgPath)\n",
    "\n",
    "#         im = imread(imgPath)\n",
    "#         img_raw_raw = im.transpose([1,2,0])\n",
    "        \n",
    "#         # img_raw = scipy.misc.imresize(img_raw_raw, (int(img_raw_raw.shape[0]/args.scale),int(img_raw_raw.shape[1]/args.scale)))\n",
    "#         img_raw = img_raw_raw\n",
    "#         print(img_raw_raw.shape,\" ->>>>\", img_raw.shape)\n",
    "\n",
    "#         labelPath = path[1]\n",
    "#         for base_x in range(0,img_raw.shape[0],framesize_h):\n",
    "#             for base_y in range(0,img_raw.shape[1],framesize_w):\n",
    "#                 for base_z in range(0,img_raw.shape[2],framesize_d):\n",
    "\n",
    "#                     if (img_raw.shape[1] - base_y < framesize_w) or (img_raw.shape[0] - base_x < framesize_h) or (img_raw.shape[2] - base_z < framesize_d):\n",
    "#                         print(\"!!!! Not adding image because size is\" , img_raw.shape[1] - base_y, img_raw.shape[0] - base_x, img_raw.shape[2] - baze_z)\n",
    "#                         continue\n",
    "\n",
    "#                     img, lab, count = getTrainingExampleCells(img_raw, framesize_w, framesize_h, framesize_d, labelPath, base_y, base_x, base_z, args.stride, args.scale)\n",
    "#                     print(\"count \", count)\n",
    "\n",
    "#                     if img.shape[0:3] != (framesize_w,framesize_h, framesize_d):\n",
    "#                         print(\"!!!! Not adding image because size is\" , img.shape[0:3])\n",
    "\n",
    "#                     else :   \n",
    "#                         lab_est = [(l.sum()/ef).astype(np.int) for l in lab]\n",
    "\n",
    "#                         print(\"lab_est \", lab_est)\n",
    "                        \n",
    "#                         assert np.allclose(count,lab_est, 0)\n",
    "\n",
    "#                         dataset.append((img,lab,count))\n",
    "\n",
    "#                         print(\"lab_est\", lab_est, \"img shape\", img.shape, \"label shape\", lab.shape)\n",
    "#                         sys.stdout.flush()\n",
    "                    \n",
    "#         print(\"dataset size\", len(dataset))\n",
    "                    \n",
    "#     print(\"writing\", datasetfilename)\n",
    "#     out = open(datasetfilename, \"wb\",0)\n",
    "#     pickle.dump(dataset, out)\n",
    "#     out.close()\n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab the data from the data set\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "np_dataset_x = np.asarray([d[0] for d in dataset], dtype=np.float32)\n",
    "np_dataset_y = np.asarray([d[1] for d in dataset], dtype=np.float32)\n",
    "np_dataset_c = np.asarray([d[2] for d in dataset], dtype=np.float32)\n",
    "\n",
    "np_dataset_x = np_dataset_x # .transpose((0,3,1,2))\n",
    "\n",
    "print(\"np_dataset_x\", np_dataset_x.shape)\n",
    "print(\"np_dataset_y\", np_dataset_y.shape)\n",
    "print(\"np_dataset_c\", np_dataset_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length = len(np_dataset_x)\n",
    "\n",
    "# 2/3 vs 1/3 for training and validation\n",
    "n = int (nsamples / 3); \n",
    "\n",
    "np_dataset_x_train = np_dataset_x[0:2*n]\n",
    "np_dataset_y_train = np_dataset_y[0:2*n]\n",
    "np_dataset_c_train = np_dataset_c[0:2*n]\n",
    "print(\"np_dataset_x_train\", len(np_dataset_x_train))\n",
    "\n",
    "np_dataset_x_valid = np_dataset_x[2*n:3*n]\n",
    "np_dataset_y_valid = np_dataset_y[2*n:3*n]\n",
    "np_dataset_c_valid = np_dataset_c[2*n:3*n]\n",
    "print(\"np_dataset_x_valid\", len(np_dataset_x_valid))\n",
    "\n",
    "np_dataset_x_test = np_dataset_x[3*n:]\n",
    "np_dataset_y_test = np_dataset_y[3*n:]\n",
    "np_dataset_c_test = np_dataset_c[3*n:]\n",
    "print(\"np_dataset_x_test\", len(np_dataset_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# length = len(np_dataset_x)\n",
    "\n",
    "# # 2/3 vs 1/3 for training and validation\n",
    "# n = 0 # int (args.nsamples / 3); \n",
    "\n",
    "# np_dataset_x_train = np_dataset_x[0:1]\n",
    "# np_dataset_y_train = np_dataset_y[0:1]\n",
    "# np_dataset_c_train = np_dataset_c[0:1]\n",
    "# print(\"np_dataset_x_train\", len(np_dataset_x_train))\n",
    "\n",
    "# np_dataset_x_valid = np_dataset_x[1:2]\n",
    "# np_dataset_y_valid = np_dataset_y[1:2]\n",
    "# np_dataset_c_valid = np_dataset_c[1:2]\n",
    "# print(\"np_dataset_x_valid\", len(np_dataset_x_valid))\n",
    "\n",
    "# np_dataset_x_test = np_dataset_x[2:]\n",
    "# np_dataset_y_test = np_dataset_y[2:]\n",
    "# np_dataset_c_test = np_dataset_c[2:]\n",
    "# print(\"np_dataset_x_test\", len(np_dataset_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some stats before the run\n",
    "print(\"number of counts total \", np_dataset_c.sum())\n",
    "print(\"number of counts on average \", np_dataset_c.mean(), \"+-\", np_dataset_c.std())\n",
    "print(\"counts min:\", np_dataset_c.min(), \"max:\", np_dataset_c.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examples of the images from the training set \n",
    "n_images_show = 7\n",
    "slice = 10\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Example images\")\n",
    "plt.imshow(np.concatenate(np_dataset_x_train[:n_images_show, :, :, slice, 0],axis=1), interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Example images\")\n",
    "slice = 10 + patch_size_d //2\n",
    "plt.imshow(np.concatenate(np_dataset_y_train[:n_images_show, :, :, slice, 0],axis=1), interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Counts in each image\")\n",
    "plt.bar(range(len(np_dataset_c_train)),np_dataset_c_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some stats again\n",
    "print(\"Total cells in training\", np.sum(np_dataset_c_train[0:], axis=0))\n",
    "print(\"Total cells in validation\", np.sum(np_dataset_c_valid[0:], axis=0))\n",
    "print(\"Total cells in testing\", np.sum(np_dataset_c_test[0:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = \"network-temp/\"\n",
    "model_ext = \"-countception-3D.h5py\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "def save_network(net, name):\n",
    "    net.save(directory + str(name) + model_ext)\n",
    "    \n",
    "# pay attention to the loss function    \n",
    "def load_network(name):\n",
    "    return load_model(directory + str(name) + model_ext, custom_objects={'mae_loss': mae_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_dataset_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "model.fit(np_dataset_x_train[:2, ], np_dataset_y_train[:2,],\n",
    "          epochs = num_epochs,\n",
    "          batch_size = batch_size*2,\n",
    "          validation_data=(np_dataset_x_valid[:2, ], np_dataset_y_valid[:2,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: is this one correct ?\n",
    "# test accuracy\n",
    "def test_perf(dataset_x, dataset_y, dataset_c):\n",
    "    testpixelerrors = []\n",
    "    testerrors = []\n",
    "    bs = 1\n",
    "    \n",
    "    # loop is unnecessary! \n",
    "    for i in range(0,len(dataset_x), bs):\n",
    "\n",
    "        pcount = model.predict(dataset_x, batch_size=4)\n",
    "        # pcount = model.predict(dataset_x,range(i,i+bs))\n",
    "        pixelerr = np.abs(pcount - dataset_y[i:i+bs]).mean(axis=(2,3))\n",
    "        testpixelerrors.append(pixelerr)\n",
    "        \n",
    "        pred_est = (pcount/(ef)).sum(axis=(1,2,3))\n",
    "        err = np.abs(dataset_c[i:i+bs].flatten()-pred_est)\n",
    "        \n",
    "        testerrors.append(err)\n",
    "    \n",
    "    return np.abs(testpixelerrors).mean(), np.abs(testerrors).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = load_network(\"test-2-gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_counts(dataset_x):\n",
    "    # batch_size = 1    \n",
    "    # batch_size not working!\n",
    "    pcount = model.predict(dataset_x.transpose([0,2,3,1]), batch_size = 4)\n",
    "    pred_est = np.sum(pcount / ef, axis=(1,2))   \n",
    "    print(pred_est.shape)\n",
    "    return pred_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Training Data\")\n",
    "\n",
    "pcounts = compute_counts(np_dataset_x_train)\n",
    "plt.bar(np.arange(len(np_dataset_c_train))-0.1,np_dataset_c_train, width=0.5, label=\"Real Count\");\n",
    "plt.bar(np.arange(len(np_dataset_c_train))+0.1,pcounts, width=0.5,label=\"Predicted Count\");\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Valid Data\")\n",
    "\n",
    "pcounts = compute_counts(np_dataset_x_valid)\n",
    "plt.bar(np.arange(len(np_dataset_c_valid))-0.1,np_dataset_c_valid, width=0.5, label=\"Real Count\");\n",
    "plt.bar(np.arange(len(np_dataset_c_valid))+0.1,pcounts, width=0.5,label=\"Predicted Count\");\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Test Data\")\n",
    "\n",
    "pcounts = compute_counts(np_dataset_x_test)\n",
    "plt.bar(np.arange(len(np_dataset_c_test))-0.1,np_dataset_c_test, width=0.5, label=\"Real Count\");\n",
    "plt.bar(np.arange(len(np_dataset_c_test))+0.1,pcounts, width=0.5,label=\"Predicted Count\");\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
