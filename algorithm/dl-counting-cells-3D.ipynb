{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3D extension of the countception network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys,os,time,random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg');\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('jet');\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage.io import imread, imsave\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KERAS stuff \n",
    "from __future__ import print_function\n",
    "import keras \n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.layers.convolutional import ZeroPadding2D, ZeroPadding3D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"keras\", keras.__version__)\n",
    "print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the backend the ordering of the channels\n",
    "print(keras.backend.backend())\n",
    "print(keras.backend.image_dim_ordering())\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup for the gpu: \n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the output of the command above\n",
    "tf.device(\"/gpu:0\")\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# this pone should help with the images of the large size\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this part is necessary to set the params from the command line\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "if len(sys.argv) == 3 and sys.argv[1] == \"-f\": #on jupyter\n",
    "    sys.argv = ['']\n",
    "    \n",
    "parser = argparse.ArgumentParser(description='Count-ception')\n",
    "\n",
    "parser.add_argument('-seed', type=int, nargs='?',default=0, help='random seed for split and init')\n",
    "parser.add_argument('-nsamples', type=int, nargs='?',default=32, help='Number of samples (N) in train and valid')\n",
    "# TODO: Is it used ? \n",
    "parser.add_argument('-stride', type=int, nargs='?',default=1, help='The args.stride at the initial layer')\n",
    "# TODO: Is it used ?\n",
    "parser.add_argument('-lr', type=float, nargs='?',default=0.00005, help='This will set the learning rate ')\n",
    "parser.add_argument('-kern', type=str, nargs='?',default=\"sq\", help='This can be gaus or sq')\n",
    "parser.add_argument('-cov', type=float, nargs='?',default=1, help='This is the covariance when kern=gaus')\n",
    "parser.add_argument('-scale', type=int, nargs='?',default=1, help='Scale the input image and labels')\n",
    "parser.add_argument('-data', type=str, nargs='?',default=\"cells\", help='Dataset folder')\n",
    "parser.add_argument('-framesize', type=int, nargs='?',default=256, help='Size of the images processed at once')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the passed parameters here if you forgot them\n",
    "args.framesize = 256\n",
    "args.scale = 1\n",
    "args.nsamples = 3\n",
    "\n",
    "print(args)\n",
    "print(keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how much to extend the initial image \n",
    "patch_size_w = int(16)\n",
    "patch_size_h = int(16)\n",
    "patch_size_d = int(16) # in the real example this one should be 8\n",
    "framesize = int(args.framesize/args.scale)\n",
    "channels = int(1)\n",
    "framesize_h = int(256)\n",
    "framesize_w = int(256)\n",
    "framesize_d = int(27) # in the real example this one should be 27\n",
    "noutputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramfilename = str(args.scale) + \"-\" + str(patch_size_w) + \"-\" + args.data + \"-\" + args.kern + str(args.cov) + \"_params.p\"\n",
    "datasetfilename = str(args.scale) + \"-\" + str(patch_size_w) + \"-\" + str(framesize) + \"-\" + args.kern + str(args.stride) + \"-\" + args.data + \"-\" + str(args.cov) + \"-dataset.p\"\n",
    "print(paramfilename)\n",
    "print(datasetfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reproducibility\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "tf.set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input shape is the image shape without the pathces\n",
    "input_shape = (framesize_w, framesize_h, framesize_d, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the input dimensions for the network\n",
    "[x + y for x, y in zip(input_shape[0:3], (patch_size_w, patch_size_h, patch_size_d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom loss funciton\n",
    "def mae_loss(y_true, y_pred):\n",
    "    # mae_loss might be too \"greedy\" and train the network on artifacts\n",
    "    # prediction_count2 = np.sum(y_pred / ef)\n",
    "    # mae_loss = K.sum(K.abs(prediction_count2 - (y_true/ef)))\n",
    "    #Mean Absolute Error is computed between each count of the count map\n",
    "    l1_loss = K.abs(y_pred - y_true)\n",
    "    loss = K.mean(l1_loss)    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom layers (building blocks)\n",
    "def ConvFactory1(data, num_filters, filter_size, stride=1, pad=(0, 0, 0), nonlinearity=LeakyReLU(alpha=0.3)):\n",
    "    # data is the input tensor, leaky rely as a nonlinearity\n",
    "    # the padding is done in the first layer automatically! \n",
    "    # no need to preprocess the data\n",
    "    data = ZeroPadding3D(padding = pad, data_format=None, input_shape=input_shape)(data)\n",
    "    data = Conv3D(filters = num_filters, kernel_size = (filter_size, filter_size, filter_size), kernel_initializer='glorot_uniform')(data)\n",
    "    data = LeakyReLU(alpha=0.3)(data)\n",
    "    data = BatchNormalization()(data)\n",
    "    return data\n",
    "    \n",
    "def SimpleFactory1(data, ch_1x1, ch_3x3):\n",
    "    # used for double layers \n",
    "    conv1x1 = ConvFactory1(data, filter_size=1, pad=0, num_filters=ch_1x1)\n",
    "    conv3x3 = ConvFactory1(data, filter_size=3, pad=1, num_filters=ch_3x3) \n",
    "    concat = Concatenate()([conv1x1, conv3x3])\n",
    "    return concat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: check again?\n",
    "def create_model(input_shape):\n",
    "    main_input = Input(shape=input_shape, name='main_input')\n",
    "    # print (net.shape)\n",
    "    net = ConvFactory1(main_input, num_filters=64, pad=(patch_size_w, patch_size_h, patch_size_d), filter_size = 3)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 16, ch_3x3 = 16)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 16, ch_3x3 = 32)\n",
    "    print (net.shape)\n",
    "    net = ConvFactory1(net, num_filters=16, filter_size = 14)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 112, ch_3x3 = 48)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 40, ch_3x3 = 40)\n",
    "    print (net.shape)\n",
    "    net = SimpleFactory1(net, ch_1x1 = 32, ch_3x3 = 96)\n",
    "    print (net.shape)\n",
    "\n",
    "    net = ConvFactory1(net, num_filters=16, filter_size = 18)\n",
    "    print (net.shape) \n",
    "    net = ConvFactory1(net, num_filters=64, filter_size = 1)\n",
    "    print (net.shape) \n",
    "    net = ConvFactory1(net, num_filters=64, filter_size = 1)\n",
    "    print (net.shape) \n",
    "    main_output = ConvFactory1(net, filter_size=1, num_filters=1)\n",
    "    print (main_output.shape)\n",
    "    \n",
    "    model = Model(inputs=[main_input], outputs = main_output)  \n",
    "    # mean_absolute_error\n",
    "    model.compile(loss=mae_loss, optimizer='sgd', metrics=['accuracy'])\n",
    "         \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_model(input_shape = input_shape)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sanity check\n",
    "print(\"network output size should be\", [x + y for x, y in zip(input_shape[0:3], (patch_size_w, patch_size_h, patch_size_d))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: FIX THE EF COEFFICINT FOR THE CASE OF CUBE \n",
    "if (args.kern == \"sq\"):\n",
    "    ef = ((patch_size_w/args.stride)**3.0)\n",
    "elif (args.kern == \"gaus\"):\n",
    "    ef = 1.0\n",
    "print(\"ef\", ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test run\n",
    "train_start_time = time.time()\n",
    "# model.fit(np.zeros([1, input_shape[0], input_shape[1], input_shape[2], input_shape[3]]), \n",
    "#          np.zeros([1, input_shape[0] + patch_size_w, input_shape[1] + patch_size_h, input_shape[2] + patch_size_d, 1]))\n",
    "print(time.time() - train_start_time, \"sec\")\n",
    "\n",
    "train_start_time = time.time()\n",
    "# model.predict();\n",
    "print(time.time() - train_start_time, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fucntions to see the results \n",
    "def genGausImage(framesize, mx, my, mz, cov=1):  \n",
    "    framesize_x = framesize[0]\n",
    "    framesize_y = framesize[1]\n",
    "    framesize_z = framesize[2]\n",
    "    \n",
    "    x, y, z = np.mgrid[0:framesize_x, 0:framesize_y, 0:framesize_z]\n",
    "    pos = np.dstack((x, y, z))\n",
    "    mean = [mx, my, mz]\n",
    "    cov = [[cov, 0, 0], [0, cov, 0], [0, 0, cov]]\n",
    "    rv = scipy.stats.multivariate_normal(mean, cov).pdf(pos)\n",
    "    return rv/rv.sum()\n",
    "\n",
    "# TODO: FIX!\n",
    "def getDensity(width, markers):\n",
    "    gaus_img = np.zeros((width,width))\n",
    "    for k in range(width):\n",
    "        for l in range(width):\n",
    "            if (markers[k,l] > 0.5):\n",
    "                gaus_img += genGausImage(len(markers),k-patch_size/2,l-patch_size/2,cov)\n",
    "    return gaus_img\n",
    "\n",
    "def getMarkersCells(labelPath, scale, size):  \n",
    "    labs = imread(labelPath).transpose([1,2,0])\n",
    "    if len(labs.shape) == 3:\n",
    "        lab = labs[:,:,:]/255\n",
    "    elif len(labs.shape) == 4:\n",
    "        lab = labs[:,:,:,0]/255\n",
    "    else:\n",
    "        print(\"unknown label format\")\n",
    "    \n",
    "    binsize = [scale, scale, scale]\n",
    "    out = np.zeros(size)    \n",
    "    for i in range(binsize[0]):\n",
    "        for j in range(binsize[1]):\n",
    "            for k in range(binsize[2]):\n",
    "                out = np.maximum(lab[i::binsize[0], j::binsize[1], k::binsize[2]], out)\n",
    "        \n",
    "    print(lab.sum(),out.sum())\n",
    "    assert np.allclose(lab.sum(),out.sum(), 1)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def getCellCountCells(markers, x, y, z, h, w, d):\n",
    "    types = [0] * noutputs\n",
    "    for i in range(noutputs):\n",
    "        types[i] = (markers[y:y+h,x:x+w, z:z+w] == 1).sum()\n",
    "        #types[i] = (markers[y:y+h,x:x+w] != -1).sum()\n",
    "    return types\n",
    "\n",
    "def getLabelsCells(markers, img_pad, base_x, base_y, base_z, stride, scale):\n",
    "    \n",
    "    height = int ((img_pad.shape[0])/args.stride)\n",
    "    width = int ((img_pad.shape[1])/args.stride)\n",
    "    depth = int ((img_pad.shape[2])/args.stride)\n",
    "    print(\"label size: \", height, width, depth)\n",
    "    labels = np.zeros((noutputs, height, width, depth))\n",
    "    if (args.kern == \"sq\"):\n",
    "        for y in range(0,height):\n",
    "            for x in range(0,width):\n",
    "                for z in range(0, depth):\n",
    "                    count = getCellCountCells(markers, x*args.stride, y*args.stride, z*args.stride, patch_size_w, patch_size_h, patch_size_d)  \n",
    "                    for i in range(0,noutputs):\n",
    "                        labels[i][y][x][z] = count[i]\n",
    "\n",
    "    \n",
    "    elif (args.kern == \"gaus\"):\n",
    "        # gauss is not checked in 3D\n",
    "        for i in range(0,noutputs):\n",
    "            labels[i] = getDensity(width, markers[base_y:base_y+width,base_x:base_x+width, base_z:base_z+depth])\n",
    "    \n",
    "    \n",
    "    print(\"getLabelsCells: DONE!\")\n",
    "    \n",
    "    count_total = getCellCountCells(markers, 0, 0, 0, framesize_h+patch_size_h, framesize_w+patch_size_w, framesize_d+patch_size_d)\n",
    "    return labels, count_total\n",
    "\n",
    "def getTrainingExampleCells(img_raw, framesize_w, framesize_h, framesize_d, labelPath, base_x,  base_y, base_z, stride, scale):\n",
    "    \n",
    "    img = img_raw[base_y:base_y+framesize_h, base_x:base_x+framesize_w, base_z:base_z+framesize_d]\n",
    "    img_pad = np.pad(img[:,:,:], ((int ((patch_size_w)/2), int ((patch_size_w)/2)), \n",
    "                                     (int ((patch_size_h)/2), int ((patch_size_h)/2)), \n",
    "                                     (int ((patch_size_d)/2), int ((patch_size_d)/2))), \"constant\")\n",
    "    \n",
    "    markers = getMarkersCells(labelPath, scale, img_raw.shape[0:3])\n",
    "    markers = markers[base_y:base_y+framesize_h, base_x:base_x+framesize_w, base_z:base_z+framesize_d]\n",
    "    markers = np.pad(markers, ((patch_size_w, patch_size_w), \n",
    "                               (patch_size_h, patch_size_h), \n",
    "                               (patch_size_d, patch_size_d)), \"constant\", constant_values=-1)\n",
    "    \n",
    "    labels, count  = getLabelsCells(markers, img_pad, base_x, base_y, base_z, args.stride, scale)\n",
    "    return img, labels, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the files from the test folder \n",
    "import glob\n",
    "\n",
    "# prefix = '/Users/kkolyva/'\n",
    "prefix = '/home/milkyklim/'\n",
    "folder = prefix + 'dl-cell-counting/algorithm/data/test-cells-3D'\n",
    "img_ext = '.tif'\n",
    "\n",
    "print('Full path:', folder)\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for filename in glob.iglob(folder + \"/*dots\" + img_ext):\n",
    "    imgg = filename.replace(\"dots\",\"cell\")\n",
    "    imgs.append([imgg,filename])\n",
    "    \n",
    "if len(imgs) == 0:\n",
    "    print(\"Issue with dataset\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to debug the data generation\n",
    "# adjust the size of the images and labels + check the output of the network\n",
    "idx = 0\n",
    "plt.rcParams['figure.figsize'] = (18, 9)\n",
    "imgPath,labelPath,x,y,z = imgs[idx][0], imgs[idx][1], 0, 0, 0\n",
    "\n",
    "print(imgPath)\n",
    "print(labelPath)\n",
    "\n",
    "im = imread(imgPath)\n",
    "print(im.shape)\n",
    "\n",
    "\n",
    "img_raw_raw = im.transpose([1,2,0]) #grayscale\n",
    "\n",
    "# plt.imshow(img_raw_raw[:,:,1,:], interpolation='none',cmap='Greys_r');\n",
    "\n",
    "print (img_raw_raw.shape[0], img_raw_raw.shape[1], img_raw_raw.shape[2])\n",
    "# size_scaled = (int(img_raw_raw.shape[0]/args.scale), \n",
    "#                int(img_raw_raw.shape[1]/args.scale), \n",
    "#                int(img_raw_raw.shape[2]/args.scale), 1)\n",
    "\n",
    "# print(size_scaled)\n",
    "# img_raw = scipy.ndimage.zoom(img_raw_raw[:,:,:,:],size_scaled)\n",
    "# img_raw = scipy.misc.imresize(img_raw_raw[:,:,:,:], size_scaled, mode='RGB')\n",
    "img_raw = img_raw_raw\n",
    "print(img_raw_raw.shape,\" ->>>>\", img_raw.shape)\n",
    "\n",
    "# plt.imshow(img_raw[:,:,1,:], interpolation='none',cmap='Greys_r');\n",
    "\n",
    "print(\"img_raw\", img_raw.shape)\n",
    "img, lab, count = getTrainingExampleCells(img_raw, framesize_w, framesize_h, framesize_d, labelPath, x, y, z, args.stride, args.scale)\n",
    "print(\"count\", count)\n",
    "\n",
    "markers = getMarkersCells(labelPath, args.scale, img_raw.shape[0:3])\n",
    "markers = markers[y:y+framesize_h, x:x+framesize_w, z:z+framesize_d]\n",
    "count = getCellCountCells(markers, 0, 0, 0, framesize_w,framesize_h, framesize_d)\n",
    "print(\"count\", count, 'markers max', markers.max())\n",
    "\n",
    "# # pcount = model.predict(np.array([img]), batch_size=1)\n",
    "\n",
    "lab_est = [(l.sum()/ef).astype(np.int) for l in lab]\n",
    "# # pred_est = [(l.sum()/ef).astype(np.int) for l in pcount]\n",
    "\n",
    "print(\"img shape\",  img.shape)\n",
    "print(\"label shape\", lab.shape)\n",
    "print(\"label est\", lab_estgetTrainingExampleCells)\n",
    "# print(\"label est \",lab_est,\" --> predicted est \",pred_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put some /show images/ here to see the result of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in imgs: \n",
    "    if (not os.path.isfile(path[0])):\n",
    "        print(path, \"bad\", path[0])\n",
    "    if (not os.path.isfile(path[1])):\n",
    "        print(path, \"bad\", path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "if (os.path.isfile(datasetfilename)):\n",
    "    print(\"reading\", datasetfilename)\n",
    "    dataset = pickle.load(open(datasetfilename, \"rb\" ))\n",
    "else:\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    dataset_c = []\n",
    "    print(len(imgs))\n",
    "    for path in imgs: \n",
    "\n",
    "        imgPath = path[0]\n",
    "        print(imgPath)\n",
    "\n",
    "        im = imread(imgPath)\n",
    "        img_raw_raw = im.transpose([1,2,0])\n",
    "        \n",
    "        # img_raw = scipy.misc.imresize(img_raw_raw, (int(img_raw_raw.shape[0]/args.scale),int(img_raw_raw.shape[1]/args.scale)))\n",
    "        img_raw = img_raw_raw\n",
    "        print(img_raw_raw.shape,\" ->>>>\", img_raw.shape)\n",
    "\n",
    "        labelPath = path[1]\n",
    "        for base_x in range(0,img_raw.shape[0],framesize_h):\n",
    "            for base_y in range(0,img_raw.shape[1],framesize_w):\n",
    "                for base_z in range(0,img_raw.shape[2],framesize_d):\n",
    "\n",
    "                    if (img_raw.shape[1] - base_y < framesize_w) or (img_raw.shape[0] - base_x < framesize_h) or (img_raw.shape[2] - base_z < framesize_d):\n",
    "                        print(\"!!!! Not adding image because size is\" , img_raw.shape[1] - base_y, img_raw.shape[0] - base_x, img_raw.shape[2] - baze_z)\n",
    "                        continue\n",
    "\n",
    "                    img, lab, count = getTrainingExampleCells(img_raw, framesize_w, framesize_h, framesize_d, labelPath, base_y, base_x, base_z, args.stride, args.scale)\n",
    "                    print(\"count \", count)\n",
    "\n",
    "                    if img.shape[0:3] != (framesize_w,framesize_h, framesize_d):\n",
    "                        print(\"!!!! Not adding image because size is\" , img.shape[0:3])\n",
    "\n",
    "                    else :   \n",
    "                        lab_est = [(l.sum()/ef).astype(np.int) for l in lab]\n",
    "\n",
    "                        print(\"lab_est \", lab_est)\n",
    "                        \n",
    "                        assert np.allclose(count,lab_est, 0)\n",
    "\n",
    "                        dataset.append((img,lab,count))\n",
    "\n",
    "                        print(\"lab_est\", lab_est, \"img shape\", img.shape, \"label shape\", lab.shape)\n",
    "                        sys.stdout.flush()\n",
    "                    \n",
    "        print(\"dataset size\", len(dataset))\n",
    "                    \n",
    "    print(\"writing\", datasetfilename)\n",
    "    out = open(datasetfilename, \"wb\",0)\n",
    "    pickle.dump(dataset, out)\n",
    "    out.close()\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab the data from the data set\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "np_dataset_x = np.asarray([d[0] for d in dataset], dtype=np.float32)\n",
    "np_dataset_y = np.asarray([d[1] for d in dataset], dtype=np.float32)\n",
    "np_dataset_c = np.asarray([d[2] for d in dataset], dtype=np.float32)\n",
    "\n",
    "np_dataset_x = np_dataset_x # .transpose((0,3,1,2))\n",
    "\n",
    "print(\"np_dataset_x\", np_dataset_x.shape)\n",
    "print(\"np_dataset_y\", np_dataset_y.shape)\n",
    "print(\"np_dataset_c\", np_dataset_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length = len(np_dataset_x)\n",
    "\n",
    "# 2/3 vs 1/3 for training and validation\n",
    "n = int (args.nsamples / 3); \n",
    "\n",
    "np_dataset_x_train = np_dataset_x[0:2*n]\n",
    "np_dataset_y_train = np_dataset_y[0:2*n]\n",
    "np_dataset_c_train = np_dataset_c[0:2*n]\n",
    "print(\"np_dataset_x_train\", len(np_dataset_x_train))\n",
    "\n",
    "np_dataset_x_valid = np_dataset_x[2*n:3*n]\n",
    "np_dataset_y_valid = np_dataset_y[2*n:3*n]\n",
    "np_dataset_c_valid = np_dataset_c[2*n:3*n]\n",
    "print(\"np_dataset_x_valid\", len(np_dataset_x_valid))\n",
    "\n",
    "np_dataset_x_test = np_dataset_x[3*n:]\n",
    "np_dataset_y_test = np_dataset_y[3*n:]\n",
    "np_dataset_c_test = np_dataset_c[3*n:]\n",
    "print(\"np_dataset_x_test\", len(np_dataset_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length = len(np_dataset_x)\n",
    "\n",
    "# 2/3 vs 1/3 for training and validation\n",
    "n = 0 # int (args.nsamples / 3); \n",
    "\n",
    "np_dataset_x_train = np_dataset_x[0:1]\n",
    "np_dataset_y_train = np_dataset_y[0:1]\n",
    "np_dataset_c_train = np_dataset_c[0:1]\n",
    "print(\"np_dataset_x_train\", len(np_dataset_x_train))\n",
    "\n",
    "np_dataset_x_valid = np_dataset_x[1:2]\n",
    "np_dataset_y_valid = np_dataset_y[1:2]\n",
    "np_dataset_c_valid = np_dataset_c[1:2]\n",
    "print(\"np_dataset_x_valid\", len(np_dataset_x_valid))\n",
    "\n",
    "np_dataset_x_test = np_dataset_x[2:]\n",
    "np_dataset_y_test = np_dataset_y[2:]\n",
    "np_dataset_c_test = np_dataset_c[2:]\n",
    "print(\"np_dataset_x_test\", len(np_dataset_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some stats before the run\n",
    "print(\"number of counts total \", np_dataset_c.sum())\n",
    "print(\"number of counts on average \", np_dataset_c.mean(), \"+-\", np_dataset_c.std())\n",
    "print(\"counts min:\", np_dataset_c.min(), \"max:\", np_dataset_c.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# examples of the images from the training set \n",
    "n_images_show = 7\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Example images\")\n",
    "plt.imshow(np.concatenate(np_dataset_x_train[:n_images_show].astype(np.uint8).transpose((0,2,3,1)),axis=1), interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.title(\"Example images\")\n",
    "plt.imshow(np.concatenate(np_dataset_y_train[:n_images_show,0],axis=1), interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.title(\"Counts in each image\")\n",
    "plt.bar(range(len(np_dataset_c_train)),np_dataset_c_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some stats again\n",
    "print(\"Total cells in training\", np.sum(np_dataset_c_train[0:], axis=0))\n",
    "print(\"Total cells in validation\", np.sum(np_dataset_c_valid[0:], axis=0))\n",
    "print(\"Total cells in testing\", np.sum(np_dataset_c_test[0:], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = \"network-temp/\"\n",
    "model_ext = \"-countception.h5py\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "def save_network(net, name):\n",
    "    net.save(directory + str(name) + model_ext)\n",
    "    \n",
    "# pay attention to the loss function    \n",
    "def load_network(name):\n",
    "    return load_model(directory + str(name) + model_ext, custom_objects={'mae_loss': mae_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "model.fit(np_dataset_x_train.transpose((0,2,3,1)), np_dataset_y_train.transpose((0,2,3,4, 1)),\n",
    "          epochs = num_epochs,\n",
    "          batch_size = batch_size,\n",
    "          validation_data=(np_dataset_x_valid.transpose((0,2,3,1)), np_dataset_y_valid.transpose((0,2,3,4,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
